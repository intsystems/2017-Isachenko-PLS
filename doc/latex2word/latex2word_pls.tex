\documentclass[12pt,twoside]{article}
\usepackage[russian,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{abstract}
\usepackage{amsmath,amssymb,mathrsfs,mathtext,amsthm}
\usepackage{a4wide}
\usepackage[T2A]{fontenc}
\usepackage{subfig}
\usepackage{url}
\usepackage[usenames]{color}
\usepackage{colortbl}

\newcommand{\hdir}{.}
\usepackage{hyperref}       % clickable links
\usepackage{lineno}
\usepackage{graphicx,multicol}
\usepackage{epstopdf}
\usepackage{cite}
\usepackage{amsmath,amssymb,mathrsfs,mathtext}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}
\newtheorem{theorem}{Theorem}
\newtheorem{statement}{Statement}
\usepackage{algorithm}
\usepackage[noend]{algcompatible}

\begin{document}
	\title
	{Dimensionality reduction for time series decoding and forecasting problems\thanks{The work was financially supported by the Russian Foundation for Basic Research (project \textcolor{red}{16-07-01155}).}}
	\date{}
	\maketitle
	\begin{center}
		R.\,V.~Isachenko\footnote{Moscow Institute of Physics and Technology, isa-ro@yandex.ru},
		M.\,R.~Vladimirova\footnote{Moscow Institute of Physics and Technology, vladimirova.maria@phystech.edu},
		V.\,V.~Strijov\footnote{A. A. Dorodnicyn Computing Centre, Federal Research Center “Computer Science and Control” of the Russian Academy of Sciences, strijov@ccas.ru}
	\end{center}
	\textbf{Abstract:} 
	The paper is devoted to the problem of decoding multiscaled time series and forecasting.
	The goal is to recover the dependence between input signal and target response.
	The proposed method allows to receive predicted values not for the next time stamp but for the whole range of values in forecast horizon.
	The prediction is multidimensional target vector instead of one timestamp point. 
	We consider the linear model of partial least squares (PLS).
	The method finds the matrix of a joint description for the design matrix and the outcome matrix.
	The obtained latent space of the joint descriptions is low-dimensional.
	This leads to a simple, stable predictive model.
	We conducted computational experiments on the real data of energy consumption and electrocorticograms signals (ECoG). 
	The experiments show significant reduction of the original spaces dimensionality and models achieve good quality of prediction.
	
	\bigskip
	\textbf{Keywords}: time series decoding, forecast, partial least squares, dimensionality reduction

\linenumbers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The paper investigates the problem of dependence recovering between an input data and a model outcome.
The proposed model is suitable for predicting a multidimensional target variable.
In the case of forecasting problem objects and target spaces have the same nature.
To build the model we need to construct autoregressive matrices for input objects and target variables.
The object is a local signal history, the outcome is signal values in the next timestamps.
An autoregressive model makes a consumption in which current signal values depend linearly on the previous signal values.

In the case of time series decoding problem objects and target spaces are different in nature, the outcome is a system response to the input signal.
The autoregressive design matrix contains local history of the input signal.
The autoregressive target matrix contains local history of the response.

The object space in time series decoding problems is high dimensional.
Excessive dimensionality of the feature description leads to the model instability.
To solve this problem the feature selection procedures are used~\cite{katrutsa2015qpfs,li2016feature}.

The paper considers the partial least squares regression (PLS) model~\cite{wegelin2000survey,abdi2003pls,geladi1986partial}.
The PLS model reduces the dimensionality of the input data and extracts the linear features combinations which have the greatest impact on the response vector.
Feature extraction is an iterative process in order of decreasing the influence on the response vector.
PLS regression methods are described in detail in~\cite{geladi1988pls, hoskuldsson1988plsr,de1993simpls}.
The difference between various PLS approaches, different kinds of the PLS regression could be found in~\cite{rosipal2006overview}.

The current state of the field and the overview of nonlinear PLS method modifications are described in~\cite{rosipal2011npls}.
A nonlinear PLS method extension was introduced in~\cite{wold1989nonlinear}.
There has been developed a variety of PLS modifications.
The proposed nonlinear PLS methods are based on smoothing splines~\cite{frank1990npls}, neural networks~\cite{qin1992npls}, radial basis functions~\cite{yan2003geneticpls}, genetic algorithms~\cite{hiden1998geneticpls}.

The result of feature selection is the dimensionality reduction and the increasing model stability without significant loss of the prediction quality.
The proposed method is used on two datasets with the redundant input and target spaces.
The first dataset consists of hourly energy consumption time series. 
Time series were collected in Poland from 1999 to 2004.

The second dataset comes from the NeuroTycho project~\cite{neurotycho} that designs brain-computer interface (BCI)~\cite{millan2010combining,mason2007comprehensive} for information transmitting between brain and electronic device.
Brain-Computer Interface (BCI) system enhances its user’s mental and physical abilities, providing a direct communication mean between brain and computer~\cite{millan2004brain}. 
BCIs aim at restoring damaged functionality of motorically or cognitively impaired patients.
The goal of motor imagery analysis is to recognize intended movements from the recorded brain activity. 
While there are various techniques for measuring cortical data for BCI~\cite{nicolas2012brain,amiri2013review}, we concentrate on the ElectroCorticoGraphic (ECoG) signals~\cite{eliseyev2016penalized}. 
ECoG, as well as other invasive techniques, provides more stable recordings and better resolution in temporal and spatial domains than its non-invasive counterparts.
We address the problem of continuous hand trajectory reconstruction. 
The subdural ECoG signals are measured across 32 channels as the subject is moving its hand.
Once the ECoG signals are transformed into informative features, the problem of trajectory reconstruction is the autoregression problem. 
Feature extraction involves application of some spectro-temporal transform to the ECoG signals from each channel~\cite{gasanov2017pls}.

In papers, which are devoted to forecasting of complex spatial time series, the forecast is built pointwise~\cite{box2015time,zhang2003time}.
If one needs to predict multiple points simultaneously, it is proposed to compute forecasted points sequentially.
During this process the previous predicted values are used to obtain a subsequent ones.
The proposed method allows to obtain multiple predicted time series values at the same time taking into account hidden dependencies not only in the object space, but also in the target space.
The proposed method significantly reduces the dimensionality of the feature space.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given a dataset $\mathfrak{D}= \left( \mathbf{X}, \mathbf{Y} \right)$, where $\mathbf{X} \in \mathbb{R}^{m \times n}$ is a design matrix, $\mathbf{Y} \in \mathbb{R}^{m \times r}$ is a target matrix. 
The examples of how to construct the dataset for a particular application task are described in the \hyperref[sec:exper]{Computational experiment}.

We assume that there is a linear dependence between the objects $\mathbf{x} \in \mathbb{R}^n$ and the responses $\mathbf{y} \in \mathbb{R}^r$
\begin{equation}
 \underset{1 \times r}{\mathbf{y}} = \underset{1 \times n}{\mathbf{x}} \cdot \underset{n \times r}{\boldsymbol{\Theta}} + \underset{1 \times r}{\boldsymbol{\varepsilon}}, 
\label{eq::model}
\end{equation}
where $\boldsymbol{\Theta}$ is the matrix of model parameters, $\boldsymbol{\varepsilon}$ is the vector of residuals.

The task is to find the matrix of the model parameters $\boldsymbol{\Theta}$ given the dataset $\mathfrak{D}$.
The optimal parameters are determined by error function minimization. 
Define the quadratic error function $S$ for the dataset $\mathfrak{D}$:
\begin{equation}
	S(\boldsymbol{\Theta} | \mathfrak{D}) = {\left\| \underset{m \times n}{\mathbf{X}} \cdot \underset{n \times r}{\boldsymbol{\Theta}} - \underset{m \times r}{\mathbf{Y}} \right\| }_2^2 = \sum_{i=1}^m \left\| \underset{1 \times n}{\mathbf{x}_i} \cdot \underset{n \times r}{\boldsymbol{\Theta}} - \underset{1 \times r}{\mathbf{y}_i} \right\|_2^2 \rightarrow\min_{\boldsymbol{\Theta}}.
\label{eq::error_function}
\end{equation}
 
 The linear dependence of the matrix $X$ columns leads to an instable solution for the optimization problem~\eqref{eq::error_function}. 
 To avoid the strong linear dependence one could use feature selection techniques.

\section{Partial Least Squares method}

To eliminate the linear dependence and reduce the dimensionality of the input space, the principal components analysis (PCA) is widely used. 
The main disadvantage of the PCA method is its insensitivity to the interrelation between the objects and the responses.
The partial least squares algorithm projects the design matrix $\mathbf{X}$ and the target matrix to the latent space $\mathbb{R}^l$ with low dimensionality ($l < r < n$).
The PLS algorithm finds the latent space matrix $\mathbf{T} \in \mathbb{R}^{m \times l}$ that best describes the original matrices $\mathbf{X}$ and $\mathbf{Y}$.

The design matrix $\mathbf{X}$ and the target matrix $\mathbf{Y}$ are projected into the latent space in the following way:
\begin{align}
\label{eq::PLS_X}
 \underset{m \times n}{\mathbf{X}} 
 &= \underset{m \times l}{\mathbf{T}} \cdot \underset{l \times n}{\mathbf{P}^{T}} + \underset{m \times n}{\mathbf{F}} 
 = \sum_{k=1}^l \underset{m \times 1}{\mathbf{t}_k} \cdot \underset{1 \times n}{\mathbf{p}_k^{T}} + \underset{m \times n}{\mathbf{F}},\\
 \label{eq::PLS_Y}
 \underset{m \times r}{\mathbf{Y}} 
 &= \underset{m \times l}{\mathbf{T}} \cdot \underset{l \times r}{\mathbf{Q}^{T}} + \underset{m \times r}{\mathbf{E}}
 =  \sum_{k=1}^l  \underset{m \times 1}{\mathbf{t}_k} \cdot \underset{1 \times r}{\mathbf{q}_k^{T}} +  \underset{m \times r}{\mathbf{E}},
\end{align}
where $\mathbf{T}$ is a joint description matrix of the objects and the outcomes in the latent space, and the columns of the matrix $\mathbf{T}$ are orthogonal; $\mathbf{P},\ \mathbf{Q}$ are transition matrices from the latent space to the original space; $\mathbf{E},\ \mathbf{F}$ are residual matrices.

The pseudocode of the PLS regression algorithm is given in the algorithm~\ref{PLSR_code}.
In each of the $l$ steps the algorithm iteratively calculates columns $\mathbf{t}_k$, $\mathbf{p}_k$, $\mathbf{q}_k$ of the matrices $\mathbf{T}$, $\mathbf{P}$, $\mathbf{Q}$, respectively. 
After the computation of the next set of vectors, the one-rank approximations are subtracted from the matrices $\mathbf{X}$, $\mathbf{Y}$. 
This step is called a matrix deflation.
In the first step one has to normalize the columns of the original matrices (subtract the mean and divide by the standard deviation).
During the test mode we need to normalize test data, compute the model prediction~\eqref{eq::model}, and then perform the reverse normalization.

\begin{algorithm}[h]
\caption{PLSR algorithm}
\label{PLSR_code}
\begin{algorithmic}[1]
	\REQUIRE $\mathbf{X}, \mathbf{Y}, l$;
	\ENSURE $\mathbf{T}, \mathbf{P}, \mathbf{Q}$;
	\STATE normalize matrices $\mathbf{X}$ и $\mathbf{Y}$ by columns
	\STATE initialize $\mathbf{u}_0$ (the first column of $\mathbf{Y}$)
	\STATE $\mathbf{X}_1 = \mathbf{X}; \mathbf{Y}_1 = \mathbf{Y}$
	\FOR{$k=1,\dots, l$}
	\REPEAT
	\vspace{0.1cm}
	\STATE $\mathbf{w}_k := \mathbf{X}_k^{T} \mathbf{u}_{k-1} / (\mathbf{u}_{k-1}^{T} \mathbf{u}_{k-1}); \quad \mathbf{w}_k: = \frac{\mathbf{w}_k}{\| \mathbf{w}_k \|}$
	\vspace{0.1cm}
	\STATE $\mathbf{t}_k := \mathbf{X}_k \mathbf{w}_k$
	\vspace{0.1cm}
	\STATE $\mathbf{c}_k := \mathbf{Y}_k^{T} \mathbf{t}_k / (\mathbf{t}_k^{T} \mathbf{t}_k); \quad \mathbf{c}_k: = \frac{\mathbf{c}_k}{\| \mathbf{c}_k \|}$
	\vspace{0.1cm}
	\STATE $\mathbf{u}_k := \mathbf{Y}_k \mathbf{c}_k$
	\UNTIL{$\mathbf{t}_k$ stabilizes}
	\vspace{0.1cm}
	\STATE $\mathbf{p}_k:= \mathbf{X}_k^{T}\mathbf{t}_k/(\mathbf{t}_k^{T}\mathbf{t}_k),\ 
	\mathbf{q}_k := \mathbf{Y}_k^{T}\mathbf{t}_k/(\mathbf{t}_k^{T}\mathbf{t}_k)$
	\vspace{0.2cm}
	\STATE $\mathbf{X}_{k+1} :=  \mathbf{X}_k - \mathbf{t}_k \mathbf{p}_k^{T}$
	\vspace{0.2cm}
	\STATE $\mathbf{Y}_{k + 1} :=  \mathbf{Y}_k - \mathbf{t}_k \mathbf{q}_k^{T}$ 
	\ENDFOR
\end{algorithmic}
\end{algorithm}

The vectors $\mathbf{t}_k$ and $\mathbf{u}_k$ from the inner loop of the algorithm~\ref{PLSR_code} contain information about the object matrix $\mathbf{X}$ and the outcome matrix $\mathbf{Y}$, respectively. 
The blocks of steps (6)--(7) and (8)-(9)~are analogues of the PCA algorithm for the matrices $\mathbf{X}$ and $\mathbf{Y}$~\cite{geladi1986partial}. 
Sequential repetition of the blocks takes into account the interaction between the matrices $\mathbf{X}$ and $\mathbf{Y}$.

The theoretical explanation of the PLS algorithm follows from the statements.
\begin{statement}
The best description of the matrices $\mathbf{X}$ and $\mathbf{Y}$ taking into account their interrelation is achieved by maximization of the covariance between the vectors $\mathbf{t}_k$ and $\mathbf{u}_k$.
\end{statement}
The statement follows from the equation
\[
\text{cov} (\mathbf{t}_k, \mathbf{u}_k) = \text{corr} (\mathbf{t}_k, \mathbf{u}_k) \cdot \sqrt{\text{var}(\mathbf{t}_k)} \cdot \sqrt{\text{var}(\mathbf{u}_k)}.
\]
Maximization of the vectors $\mathbf{t}_k$ and $\mathbf{u}_k$ variances corresponds to keeping information about original matrices, the correlation of these vectors corresponds to interrelation between $\mathbf{X}$ and~$\mathbf{Y}$. $\blacksquare$

In the inner loop of the algorithm~\ref{PLSR_code} the normalized weight vectors $\mathbf{w}_k$ and $\mathbf{c}_k$ are calculated. 
These vectors construct the matrices $\mathbf{W}$ and $\mathbf{C}$, respectively.

\begin{statement}
	The vector $\mathbf{w}_k$ and $\mathbf{c}_k$ are eigenvectors of the matrices $\mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{Y}_k^{T} \mathbf{X}_k$ and $\mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{X}_k^{T} \mathbf{Y}_k$, corresponding to the maximum eigenvalues.
	
	\begin{equation*}
	\mathbf{w}_k \varpropto \mathbf{X}_k^{T} \mathbf{u}_{k-1} \varpropto \mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{c}_{k-1} \varpropto \mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{Y}_k^{T} \mathbf{t}_{k-1} \varpropto \mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{w}_{k-1},
	\end{equation*}
	\begin{equation*}
	\mathbf{c}_k \varpropto \mathbf{Y}_k^{T} \mathbf{t}_k \varpropto \mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{w}_k \varpropto \mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{X}_k^{T} \mathbf{u}_{k-1} \varpropto \mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{c}_{k-1},
	\end{equation*}
	where the $\varpropto$ symbol means equality up to a multiplicative constant.
	\label{st::eig}
\end{statement}

The statement follows from the fact that the update rule for vectors $\mathbf{w}_k$, $\mathbf{c}_k$ coincides with the iteration of the power method for the maximum eigenvalue.

Let a matrix $\mathbf{A}$ be diagonalizable, $\mathbf{x}$ be some vector, then
\[
	\lim_{k \rightarrow \infty} \mathbf{A}^k \mathbf{x} = \lambda_{\max}(\mathbf{A}) \cdot \mathbf{v}_{\max},
\]
where $ \lambda_{\max} (\mathbf{A})$~is the maximum eigenvalue of the matrix $\mathbf{A}$, $\mathbf{v}_{\max}$~is the eigenvector of the matrix $\mathbf{A}$, corresponding to $ \lambda_{\max} (\mathbf{A})$.
$\blacksquare$

\begin{statement}

The update rule for the vectors in steps (6)--(9) of the algorithm~\ref{PLSR_code} corresponds to the maximization of the covariance between the vectors $\mathbf{t}_k$ and $\mathbf{u}_k$.
\end{statement}

The maximum covariance between the vectors $\mathbf{t}_k$ and $\mathbf{u}_k$ is equal to the maximum eigenvalue of the matrix $\mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{Y}_k^{T} \mathbf{X}_k$:
\begin{align*}
\max_{\mathbf{t}_k, \mathbf{u}_k}  \text{cov} (\mathbf{t}_k, \mathbf{u}_k)^2 &= \max_{\substack{\|\mathbf{w}_k\|=1 \\ \|\mathbf{c}_k\| = 1}} \text{cov} \left( \mathbf{X}_k \mathbf{w}_k, \mathbf{Y}_k \mathbf{c}_k \right)^2 = \max_{\substack{\|\mathbf{w}_k\|=1 \\ \|\mathbf{c}_k\| = 1}} \text{cov} \left(\mathbf{c}_k^{T}  \mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{w}_k \right)^2 = \\
&= \max_{\|\mathbf{w}_k\| = 1} \text{cov} \left\|\mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{w}_k \right\|^2 = \max_{\|\mathbf{w}_k\| = 1} \mathbf{w}_k^{T} \mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{Y}_k^{T} \mathbf{X}_k \mathbf{w}_k = \\
& = \lambda_{\max} \left( \mathbf{X}_k^{T} \mathbf{Y}_k \mathbf{Y}_k^{T} \mathbf{X}_k \right),
\end{align*}
where $ \lambda_{\max} (\mathbf{A})$~is the maximum eigenvalue of the matrix $\mathbf{A}$.
Using the statement~\ref{st::eig}, we obtain the required result.
$\blacksquare$

After the inner loop the following step (11) is to compute vectors $\mathbf{p}_k$, $\mathbf{q}_k$ by projection of the matrices $\mathbf{X}_k$ and $\mathbf{Y}_k$ columns to the vector $\mathbf{t}_k$. 
To go to the next step one has to deflate the matrices $\mathbf{X}_k$ and $\mathbf{Y}_k$ by the one-rank approximations $\mathbf{t}_k \mathbf{p}_k^{T}$ and $\mathbf{t}_k \mathbf{q}_k^{T}$
\begin{align*}
    \mathbf{X}_{k + 1} &= \mathbf{X}_{k} - \mathbf{t}_k \mathbf{p}_k^{T} = \mathbf{X} - \sum_k \mathbf{t}_k \mathbf{p}_k^{T}, \\
    \mathbf{Y}_{k + 1} &= \mathbf{Y}_{k} - \mathbf{t}_k \mathbf{q}_k^{T} = \mathbf{Y} - \sum_k \mathbf{t}_k \mathbf{q}_k^{T}.
\end{align*}
Each next vector $\mathbf{t}_{k+1}$ turns out to be orthogonal to all vectors $\mathbf{t}_i$, $i=1, \dots, k$.

Let assume that the dimension of object, response, and latent variable spaces are equal to~2 ($n = r = l = 2$).
Fig.~\ref{fig::PLSFigure} shows the result of the PLS algorithm in this case.
Blue and green dots represent the rows of the matrices $\mathbf{X}$ and $\mathbf{Y}$, respectively. 
The dots were generated from a normal distribution with zero expectation. 
Contours of the distribution covariance matrices are shown in red.
Black contours are unit circles. 
Red arrows correspond to principal components for the set of points. 
Black arrows correspond to the vectors of the matrices $\mathbf{W}$ and $\mathbf{C}$ from the PLS algorithm. 
The vectors $\mathbf{t}_k$ and $\mathbf{u}_k$ are equal to the projected matrices $\mathbf{X}_k$ and $\mathbf{Y}_k$ to the vectors $\mathbf{w}_k$ and $\mathbf{c}_k$, respectively, and are denoted by black pluses. 
Taking into account the interaction between the matrices $\mathbf{X}$ and $\mathbf{Y}$ the vectors $\mathbf{w}_k$ and $\mathbf{c}_k$ deviate from the principal components directions. 
The deviation of the vectors $\mathbf{w}_k$ is insignificant. 
In the first iteration, $\mathbf{c}_1$ is close to the principal component $\textit{pc}_1$, but the vectors $\mathbf{c}_k$ in the next iterations could strongly correlate. 
The difference in the vectors $\mathbf{w}_k$ and $\mathbf{c}_k$ the behaviour is associated with the deflation process. In particular, we subtract from $\mathbf{Y}$ the one-rank approximation found in the space of the matrix $\mathbf{X}$.

To obtain the model predictions and find the model parameters, let multiply the both hand sides of the equation~\eqref{eq::PLS_X} by the matrix $\mathbf{W}$. 
Since the residual matrix  $\mathbf{E}$ rows are orthogonal to the columns of the matrix $\mathbf{W}$, we have
\[
	\mathbf{X} \mathbf{W} = \mathbf{T} \mathbf{P}^{T} \mathbf{W}.
\]
The linear transformation between objects in the input and latent spaces has the form
\begin{equation}
	\mathbf{T} = \mathbf{X} \mathbf{W}^*,
	\label{eq::W*}
\end{equation}
where $\mathbf{W}^* = \mathbf{W} (\mathbf{P}^{T} \mathbf{W})^{-1}$.

The matrix of the model parameters~\ref{eq::model} could be found from equations~\eqref{eq::PLS_Y},~\eqref{eq::W*}
\begin{equation*}
    \mathbf{Y} = \mathbf{T} \mathbf{Q}^{T} + \mathbf{E} = \mathbf{X} \mathbf{W}^* \mathbf{Q}^{T} + \mathbf{E} = \mathbf{X} \boldsymbol{\Theta} + \mathbf{E}.
\end{equation*}
Thus, the model parameters~\eqref{eq::model} are equal to
\begin{equation}
    \boldsymbol{\Theta} = \mathbf{W} (\mathbf{P}^{T} \mathbf{W})^{-1} \mathbf{Q}^{T}.
    \label{eq::model_parameters}
\end{equation}

To find the model predictions during the testing we have to
\begin{itemize}
	\item normalize the test data;
	\item compute the prediction of the model using the linear transformation with the matrix $\boldsymbol{\Theta}$ from~\eqref{eq::model_parameters};
	\item perform the inverse normalization.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computational experiment}
\label{sec:exper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Time series of energy consumption contain hourly records (total of 52 512 observations). 
The matrix $\mathbf{X}$ row is the local history of the signal for one week $n = 24 \times 7$. 
The matrix $\mathbf{Y}$ row is the local forecast of energy consumption for the next 24 hours $r = 24$. 
In this case, the matrices $\mathbf{X}$ and $\mathbf{Y}$ are autoregressive matrices.

In the ECOG data case, the matrix $\mathbf{X}$ consists of the spatial-temporal representation of voltage time series, and the matrix $\mathbf{Y}$ contains information about the hand position.
The generation process of the matrix $\mathbf{X}$ from the voltage values is described in~\cite{gasanov2017pls}. 
Feature description in each time moment has dimension equal to $864$. The hand position is described by the coordinates along three axes. 
An example of voltage data samples with the different channels and corresponding spatial coordinates of the hand are shown in Fig.~\ref{fig::ecog_data}.
To predict the hand position in the next moments we used an autoregressive approach.
One object consists of a feature description in a few moments. 
The answer is the hand position in the next moments of time.
The task is to predict the hand position in the next few moments of time.

We introduce the mean-squared error for matrices $\mathbf{A} = [a_{ij}]$ and $\mathbf{B} = [b_{ij}]$ 
\[
\text{MSE} (\mathbf{A}, \mathbf{B}) = \sum_{i,j} (a_{ij} - b_{ij})^2.
\]
To estimate the prediction quality, we compute the normalized MSE 
\begin{equation}
\text{NMSE}(\mathbf{Y},  \mathbf{\hat{Y}}) = \frac{\text{MSE} (\mathbf{Y}, \mathbf{\hat{Y}})}{\text{MSE} (\mathbf{Y}, \mathbf{\bar{Y}})},
\label{eq::nmse}
\end{equation}
where $\mathbf{\hat{Y}}$ is the model outcome, $\mathbf{\bar{Y}}$ is the average constant forecast over the columns of the matrix.

\subsection{Energy consumption dataset}

To find the optimal dimensionality $l$ of the latent space, the energy consumption dataset was divided into training and validation parts. 
The training data consists of $700$ objects, the validation data~--- of $370$. The dependence of the normalized mean-squared error~\eqref{eq::nmse} on the latent space with dimensionality $l$ is shown in Fig.~\ref{fig::energy_n_comp}. 
First, the error drops sharply with increasing the latent space dimensionality and then changes slightly.


The error achieves the minimum value for $l=14$. 
Let us build a forecast of energy consumption for a given $l$. 
The result is shown in Fig.~\ref{fig::energy_prediction}. 
The PLS algorithm restored the autoregressive dependence and found the daily seasonality.

\subsection{ECoG dataset}
Fig.~\ref{fig::ecog_n_comp} illustrates the dependence of the normalized mean-squared error~\eqref{eq::nmse} on the latent space dimensionality $l$ for ECoG dataset. 
The approximation error changes slightly for $l > 5$.
The joint spatial-temporal representation of objects and the hand position can be represented as a vector of dimensionality equal to $l \ll n$.
Let us fix $l = 5$. 
An example of the hand position approximation is shown in Fig.~\ref{fig::ecog_prediction}. 
Solid lines represent the true coordinates of the hand along all axes, the dotted lines show the approximation by the PLS algorithm.
 
\section{Conclusion}
In the paper we proposed the approach for solving the problem of time series decoding and forecasting. 
The algorithm of partial least squares allows to build a simple, stable, and linear model. 
The obtained latent space gathers information about the objects and the responses and dramatically reduces the dimensionality of the input matrices. 
The computational experiment demonstrated the proposed method applicability to the tasks of electricity consumption forecasting and brain-computer interface designing.
The future research will be aimed to the extension of the proposed method for the non-linear dependencies class.

\bibliographystyle{unsrt}
\begin{thebibliography}{10}
	
	\bibitem{katrutsa2015qpfs}
	AM~Katrutsa and VV~Strijov.
	\newblock Stress test procedure for feature selection algorithms.
	\newblock {\em Chemometrics and Intelligent Laboratory Systems}, 142:172--183,
	2015.
	
	\bibitem{li2016feature}
	Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert~P Trevino,
	Jiliang Tang, and Huan Liu.
	\newblock Feature selection: A data perspective.
	\newblock {\em arXiv preprint arXiv:1601.07996}, 2016.
	
	\bibitem{wegelin2000survey}
	Jacob~A Wegelin et~al.
	\newblock A survey of partial least squares (pls) methods, with emphasis on the
	two-block case.
	\newblock {\em University of Washington, Department of Statistics, Tech. Rep},
	2000.
	
	\bibitem{abdi2003pls}
	Herv{\'{e}} Abdi.
	\newblock {Partial Least Squares (PLS) Regression}.
	\newblock {\em Encyclopedia for research methods for the social sciences},
	pages 792--795, 2003.
	
	\bibitem{geladi1986partial}
	Paul Geladi and Bruce~R Kowalski.
	\newblock Partial least-squares regression: a tutorial.
	\newblock {\em Analytica chimica acta}, 185:1--17, 1986.
	
	\bibitem{geladi1988pls}
	Paul Geladi.
	\newblock {Notes on the history and nature of partial least squares (PLS)
		modelling}.
	\newblock {\em Journal of Chemometrics}, 2(January):231--246, 1988.
	
	\bibitem{hoskuldsson1988plsr}
	Agnar H{\"{o}}skuldsson.
	\newblock {PLS regression.}
	\newblock {\em Journal of Chemometrics}, 2(August 1987):581--591, 1988.
	
	\bibitem{de1993simpls}
	Sijmen De~Jong.
	\newblock Simpls: an alternative approach to partial least squares regression.
	\newblock {\em Chemometrics and intelligent laboratory systems},
	18(3):251--263, 1993.
	
	\bibitem{rosipal2006overview}
	Roman Rosipal and Nicole Kramer.
	\newblock {Overview and Recent Advances in Partial Least Squares}.
	\newblock {\em C. Saunders et al. (Eds.): SLSFS 2005, LNCS 3940}, pages 34--51,
	2006.
	
	\bibitem{rosipal2011npls}
	Roman Rosipal.
	\newblock {Nonlinear partial least squares: An overview}.
	\newblock {\em Chemoinformatics and Advanced Machine Learning Perspectives:
		Complex Computational Methods and Collaborative Techniques}, pages 169--189,
	2011.
	
	\bibitem{wold1989nonlinear}
	Svante Wold, Nouna Kettaneh-Wold, and Bert Skagerberg.
	\newblock Nonlinear pls modeling.
	\newblock {\em Chemometrics and Intelligent Laboratory Systems}, 7(1-2):53--65,
	1989.
	
	\bibitem{frank1990npls}
	Ildiko~E. Frank.
	\newblock {A nonlinear PLS model}.
	\newblock {\em Chemometrics and Intelligent Laboratory Systems}, 8(2):109--119,
	1990.
	
	\bibitem{qin1992npls}
	S~Joe Qin and Thomas~J McAvoy.
	\newblock Nonlinear pls modeling using neural networks.
	\newblock {\em Computers \& Chemical Engineering}, 16(4):379--391, 1992.
	
	\bibitem{yan2003geneticpls}
	Xuefeng~F. Yan, Dezhao~Z. Chen, and Shangxu~X. Hu.
	\newblock {Chaos-genetic algorithms for optimizing the operating conditions
		based on RBF-PLS model}.
	\newblock {\em Computers and Chemical Engineering}, 27(10):1393--1404, 2003.
	
	\bibitem{hiden1998geneticpls}
	Hugo Hiden, Ben McKay, Mark Willis, and Gary Montague.
	\newblock Non-linear partial least squares using genetic.
	\newblock In {\em Genetic Programming 1998: Proceedings of the Third}, pages
	128--133. Morgan Kaufmann, 1998.
	
	\bibitem{neurotycho}
	Project tycho
	\href{http://neurotycho.org/food-tracking-task}{http://neurotycho.org/food-tracking-task}.
	
	\bibitem{millan2010combining}
	Jos{\'e} del~R Mill{\'a}n, R{\"u}diger Rupp, Gernot Mueller-Putz, Roderick
	Murray-Smith, Claudio Giugliemma, Michael Tangermann, Carmen Vidaurre, Febo
	Cincotti, Andrea Kubler, Robert Leeb, et~al.
	\newblock Combining brain--computer interfaces and assistive technologies:
	State-of-the-art and challenges.
	\newblock {\em Frontiers in Neuroscience}, 4:161, 2010.
	
	\bibitem{mason2007comprehensive}
	SG~Mason, A~Bashashati, M~Fatourechi, KF~Navarro, and GE~Birch.
	\newblock A comprehensive survey of brain interface technology designs.
	\newblock {\em Annals of biomedical engineering}, 35(2):137--169, 2007.
	
	\bibitem{millan2004brain}
	Jos{\'e} del~R Mill{\'a}n, Fr{\'e}d{\'e}ric Renkens, Josep Mouri{\~n}o, and
	Wulfram Gerstner.
	\newblock Brain-actuated interaction.
	\newblock {\em Artificial Intelligence}, 159(1-2):241--259, 2004.
	
	\bibitem{nicolas2012brain}
	Luis~Fernando Nicolas-Alonso and Jaime Gomez-Gil.
	\newblock Brain computer interfaces, a review.
	\newblock {\em Sensors}, 12(2):1211--1279, 2012.
	
	\bibitem{amiri2013review}
	Setare Amiri, Reza Fazel-Rezai, and Vahid Asadpour.
	\newblock A review of hybrid brain-computer interface systems.
	\newblock {\em Advances in Human-Computer Interaction}, 2013:1, 2013.
	
	\bibitem{eliseyev2016penalized}
	Andrey Eliseyev and Tetiana Aksenova.
	\newblock Penalized multi-way partial least squares for smooth trajectory
	decoding from electrocorticographic (ecog) recording.
	\newblock {\em PloS one}, 11(5):e0154878, 2016.
	
	\bibitem{gasanov2017pls}
	Motrenko~A. Gasanov~I.
	\newblock Creation of approximating scalogram description in a problem of
	movement prediction.
	\newblock {\em Journal of Machine Learning and Data Analysis}, 3(2):160--169,
	2017.
	
	\bibitem{box2015time}
	George~EP Box, Gwilym~M Jenkins, Gregory~C Reinsel, and Greta~M Ljung.
	\newblock {\em Time series analysis: forecasting and control}.
	\newblock John Wiley \& Sons, 2015.
	
	\bibitem{zhang2003time}
	G~Peter Zhang.
	\newblock Time series forecasting using a hybrid arima and neural network
	model.
	\newblock {\em Neurocomputing}, 50:159--175, 2003.
	
\end{thebibliography}


\end{document}